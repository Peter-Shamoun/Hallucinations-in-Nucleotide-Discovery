{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI  #OpenAI API client\n",
    "from dotenv import load_dotenv  #For loading environment variables from .env file\n",
    "load_dotenv()  #This loads variables from a .env file into environment variables\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  #Get the OpenAI API key from environment variables\n",
    "client = OpenAI(api_key=api_key)  #Initialize the OpenAI client with the API key\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored enhancer with 98 genes.\n",
      "Stored methylated with 100 genes.\n",
      "Stored non_enhancer with 98 genes.\n",
      "Stored non_methylated with 100 genes.\n",
      "Stored non_promoter with 100 genes.\n",
      "Stored non_protein_coding with 88 genes.\n",
      "Stored non_splice_site with 70 genes.\n",
      "Stored promoter with 100 genes.\n",
      "Stored protein_coding with 100 genes.\n",
      "Stored splice_site with 70 genes.\n"
     ]
    }
   ],
   "source": [
    "fasta_dir = \"sequences\"\n",
    "\n",
    "# Mapping of base categories to questions\n",
    "questions = {\n",
    "    \"protein_coding\": \"Does this nucleotide sequence encode a protein? Only answer Yes or No. You must start your answer with 'Yes' or 'No'.\",\n",
    "    \"enhancer\": \"Does this nucleotide sequence function as an enhancer in gene regulation? Only answer Yes or No. You must start your answer with 'Yes' or 'No'.\",\n",
    "    \"promoter\": \"Does this nucleotide sequence act as a promoter for transcription initiation? Only answer Yes or No. You must start your answer with 'Yes' or 'No'.\",\n",
    "    \"splice_site\": \"Does this nucleotide sequence contain a splice site for RNA processing? Only answer Yes or No. You must start your answer with 'Yes' or 'No'.\",\n",
    "    \"methylated\": \"Is this nucleotide sequence methylated as part of epigenetic regulation? Only answer Yes or No. You must start your answer with 'Yes' or 'No'.\"\n",
    "}\n",
    "\n",
    "# Dictionary to hold sequence data\n",
    "sequence_arrays = {}\n",
    "\n",
    "# Loop through all FASTA files in the directory\n",
    "for file in os.listdir(fasta_dir):\n",
    "    if file.endswith(\".fasta\"):  # Process only FASTA files\n",
    "        file_path = os.path.join(fasta_dir, file)\n",
    "        var_name = os.path.splitext(file)[0]  # Use file name (without extension) as key\n",
    "\n",
    "        # Determine base category\n",
    "        if var_name.startswith(\"non_\"):\n",
    "            base_category = var_name[4:]  # Remove \"non_\" prefix\n",
    "        else:\n",
    "            base_category = var_name\n",
    "        question = questions[base_category]  # Get the corresponding question\n",
    "\n",
    "        # Read the FASTA file and create a list of dictionaries\n",
    "        gene_dicts = []\n",
    "        for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "            gene_dict = {\n",
    "                \"file_name\": var_name,\n",
    "                \"gene\": str(record.seq),\n",
    "                \"question\": question,  # Add the question key\n",
    "            }\n",
    "            gene_dicts.append(gene_dict)\n",
    "        \n",
    "        # Store the list of dictionaries in the dictionary\n",
    "        sequence_arrays[var_name] = gene_dicts\n",
    "\n",
    "        print(f\"Stored {var_name} with {len(gene_dicts)} genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hallucination(gene_dict, model):\n",
    "    response = client.chat.completions.create(\n",
    "            model= model,  #Use the model specified in the constructor\n",
    "\n",
    "            messages=\n",
    "            [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in genomics.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{gene_dict['gene']} Describe the gene in natural language:\"}\n",
    "        ],  #The conversation context to send to the API\n",
    "\n",
    "\n",
    "            max_tokens=256,  #Maximum length of the response\n",
    "\n",
    "            temperature=0.6  #Controls randomness/creativity of the response\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification(gene_dict, model):\n",
    "    response = client.chat.completions.create(\n",
    "            model = model,  #Use the model specified in the constructor\n",
    "\n",
    "            messages=\n",
    "            [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in genomics.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{gene_dict['gene']} {gene_dict['question']}\"}\n",
    "        ],  #The conversation context to send to the API\n",
    "\n",
    "\n",
    "            max_tokens=256,  #Maximum length of the response\n",
    "\n",
    "            temperature=0.6  #Controls randomness/creativity of the response\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hallucination_classification(gene_dict, model):\n",
    "    response = client.chat.completions.create(\n",
    "            model = model,  #Use the model specified in the constructor\n",
    "\n",
    "            messages=\n",
    "            [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in genomics.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{gene_dict['gene']} {gene_dict['hallucination']} {gene_dict['question']}\"}\n",
    "        ],  #The conversation context to send to the API\n",
    "\n",
    "\n",
    "            max_tokens=256,  #Maximum length of the response\n",
    "\n",
    "            temperature=0.6  #Controls randomness/creativity of the response\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Model [1/6]: gpt-3.5-turbo:   0%|          | 0/10 [00:20<?, ?it/s]\n",
      "File [1/10]: enhancer:  10%|â–ˆ         | 10/98 [00:20<02:54,  1.99s/it, Gene 11/98]"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "MODELS = [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4o-mini\",\n",
    "\n",
    "]\n",
    "for model_idx, model in enumerate(MODELS):\n",
    "    model_progress = tqdm(total=len(sequence_arrays), desc=f\"Model [{model_idx+1}/6]: {model.split('/')[-1]}\")\n",
    "    \n",
    "    for file_idx, (file, sequences) in enumerate(sequence_arrays.items()):\n",
    "        file_progress = tqdm(total=len(sequences), desc=f\"File [{file_idx+1}/{len(sequence_arrays)}]: {file}\", leave=False)\n",
    "        \n",
    "        for gene_idx, gene in enumerate(sequences):\n",
    "            file_progress.set_postfix_str(f\"Gene {gene_idx+1}/{len(sequences)}\")\n",
    "            \n",
    "            result = gene.copy()\n",
    "            result['model'] = model\n",
    "            \n",
    "            try:\n",
    "                result['hallucination'] = generate_hallucination(result, model)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating hallucination for {file}, gene {gene_idx+1}: {str(e)}\")\n",
    "                result['hallucination'] = None\n",
    "            \n",
    "            try:\n",
    "                result['hallucination_classification'] = generate_hallucination_classification(result, model)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating hallucination classification for {file}, gene {gene_idx+1}: {str(e)}\")\n",
    "                result['hallucination_classification'] = None\n",
    "            \n",
    "            try:\n",
    "                result['classification'] = generate_classification(result, model)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating classification for {file}, gene {gene_idx+1}: {str(e)}\")\n",
    "                result['classification'] = None\n",
    "            \n",
    "            all_results.append(result)\n",
    "            \n",
    "            file_progress.update(1)\n",
    "        \n",
    "        file_progress.close()\n",
    "        model_progress.update(1)\n",
    "    \n",
    "    model_progress.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results)\n",
    "df['correct'] = df['file_name'].str.contains(\"non\", case=True, na=False).map({True: \"No\", False: \"Yes\"})\n",
    "df['hallucination_classification'] = df['hallucination_classification'].str.replace(\".\", \"\", regex=False)\n",
    "df['classification'] = df['classification'].str.replace(\".\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hallucination_classification'] = df['hallucination_classification'].apply(\n",
    "    lambda x: 'Yes' if isinstance(x, str) and x.split()[0].lower() == 'yes' else\n",
    "              'No' if isinstance(x, str) and x.split()[0].lower() == 'no' else np.nan\n",
    ")\n",
    "print(df['hallucination_classification'].values)\n",
    "\n",
    "df['classification'] = df['classification'].apply(\n",
    "    lambda x: 'Yes' if isinstance(x, str) and x.split()[0].lower() == 'yes' else\n",
    "              'No' if isinstance(x, str) and x.split()[0].lower() == 'no' else np.nan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "df['hallucination_correct'] = df['hallucination_classification'] == df['correct']\n",
    "df['no_hallucination_correct'] = df['classification'] == df['correct']\n",
    "df.to_csv(\"trail_1_GPT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_results = df.groupby('model')[['hallucination_correct', 'no_hallucination_correct']].mean()\n",
    "grouped_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
